import requests
from urllib.parse import urlparse
from bs4 import BeautifulSoup
# âœ… è«‹æ›¿æ›æˆä½ çš„ Perplexity API é‡‘é‘°
API_KEY = "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"
API_URL = "https://api.perplexity.ai/chat/completions"

industry = "æ‰‹æ©Ÿç”¢æ¥­"

# ä½ è¦å…è¨±çš„ç¶²ç«™
allowed_domains = [

    "analyticsvidhya.com",
    "thaubing.gcaa.org.tw",
    "cbinsights.com",
    "eetimes.itmedia.co.jp",
    "techanalye.com",
    # "scribd.com",
]

# ç”¢ç”Ÿ Google æœå°‹æŒ‡ä»¤
google_queries = "\n".join(
    [f"site:{domain} {industry} æœ€æ–°æ¶ˆæ¯ OR å ±å‘Š OR è¶¨å‹¢" for domain in allowed_domains]
)

prompt = f"""
ä»Šå¹´æ˜¯2025å¹´ï¼Œè€Œä½ æ˜¯ä¸€ä½50å¹´ç¶“é©—çš„å°ˆæ¥­ç”¢æ¥­åˆ†æå¸«ï¼Œè«‹å³æ™‚æ”¶é›†ã€Œ{industry}ã€çš„æœ€æ–°å…¬é–‹è³‡è¨Šï¼Œé€²è¡Œç”¢æ¥­åˆ†æã€‚
è«‹åŒ…å«ï¼š
1. ç¾æ³ç¸½è¦½
2. ä¸»è¦å…¬å¸ï¼ˆåœ‹å…§å¤–ï¼‰
3. æœªä¾†ç™¼å±•è¶¨å‹¢ï¼ˆè‡³å°‘ 2 å¹´ï¼‰
4. æ½›åœ¨é¢¨éšªèˆ‡æŒ‘æˆ°
5. æˆé•·è¶¨å‹¢ JSON æ ¼å¼è³‡æ–™ï¼ˆä¾‹å¦‚ï¼š[{{"year": 2023, "market_cap": 1200}}]ï¼‰

ä¸¦ä¸”ï¼š
- çµ¦æˆ‘è¿‘ä¸€å‘¨æ‰€ç™¼ç”Ÿçš„ç›¸é—œåœ‹éš›å¤§äº‹ï¼Œä¾æ—¥æœŸåˆ—å‡º
- æ‰€æœ‰è³‡è¨Šä¾†æºå¿…é ˆ**åƒ…é™æ–¼ä»¥ä¸‹ç¶²ç«™**ï¼š
  {", ".join(allowed_domains)}
- å¦‚æœç›´æ¥è¨ªå•é€™äº›ç¶²ç«™çš„å…§å®¹éœ€è¦è¨»å†Šæˆ–ç„¡æ³•å–å¾—ï¼Œè«‹æ”¹ç”¨ Google æœå°‹ï¼š
{google_queries}
- æœå°‹çµæœå¿…é ˆä¾†è‡ªä¸Šè¿°ç¶²ç«™çš„å…¬é–‹é é¢ï¼Œä¸å¾—å¼•ç”¨å…¶ä»–ç¶²ç«™
- æ¯æ®µå…§å®¹æœ€å¾Œéœ€é™„ä¸ŠåŸå§‹ URL
"""

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

data = {
    "model": "sonar-pro",
    "messages": [
        {"role": "system", "content": "ä½ æ˜¯çŸ¥è­˜æ·µåšçš„ç”¢æ¥­åˆ†æå¸«"},
        {"role": "user", "content": prompt}
    ],
    "enable_search_classifier": True,
    "search_mode": "web",
    "search_domain_filter": allowed_domains  # âœ… åªæœƒæŠ“å–é€™äº›ç¶²ç«™
}

response = requests.post(API_URL, headers=headers, json=data)

if response.status_code == 200:
    content = response.json()["choices"][0]["message"]["content"]
    print("ğŸ“˜ Perplexity å›è¦†ï¼š\n")
    print(content)
else:
    print("âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š", response.status_code)
    print(response.text)
if response.status_code != 200:
    print("âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š", response.status_code)
    print(response.text)
    exit()

content = response.json()["choices"][0]["message"]["content"]
print("ğŸ“˜ Perplexity å›è¦†ï¼š\n")
print(content)

# å˜—è©¦å¾ JSON å›å‚³ä¸­æå–ä¾†æº URL
try:
    citations = response.json().get("citations", [])
except Exception:
    citations = []

# å¦‚æœ citations ç©ºï¼Œå¾å…§å®¹ä¸­å˜—è©¦æŠ“ URL
import re
if not citations:
    citations = re.findall(r'https?://[^\s\)]+', content)

# é©—è­‰ä¾†æºç¶²å€æ˜¯å¦åœ¨å…è¨±æ¸…å–®ä¸­
def is_allowed(url):
    domain = urlparse(url).netloc
    return any(allowed_domain in domain for allowed_domain in allowed_domains)

bad_sources = [src for src in citations if not is_allowed(src)]
if bad_sources:
    print("\nâš ï¸ ç™¼ç¾ä¸åœ¨å…è¨±æ¸…å–®çš„ä¾†æºï¼š", bad_sources)
else:
    print("\nâœ… æ‰€æœ‰ä¾†æºçš„ domain éƒ½åœ¨å…è¨±æ¸…å–®å…§")

# é©—è­‰å…§å®¹æ˜¯å¦çœŸçš„å­˜åœ¨æ–¼è©² URL
def verify_content(url, snippet):
    try:
        html = requests.get(url, timeout=10).text
        soup = BeautifulSoup(html, "html.parser")
        return snippet[:50] in soup.get_text()
    except Exception:
        return False

print("\nğŸ” é©—è­‰ä¾†æºå…§å®¹...")
for url in citations:
    if is_allowed(url):
        snippet_match = verify_content(url, content)
        print(f"{url} â†’ {'âœ… å…§å®¹åŒ¹é…' if snippet_match else 'âš ï¸ å…§å®¹å¯èƒ½ä¸ä¸€è‡´'}")